import numpy as np
import mindspore_lite as mslite
from mindspore import context
import mindspore.dataset as ds
import mindspore.dataset.vision as vision
import os
import time
import sys
from tqdm import tqdm

# è®¾ç½®ç¯å¢ƒå˜é‡ - ä¼˜åŒ–NPUè¿è¡Œ
os.environ['TUNE_OPS_ONLINE'] = '0'
os.environ['MS_BUILD_PROCESS_NUM'] = '1'
os.environ['OPTION_EXEC_PARTITION_OPS'] = '1'
os.environ['ASCEND_GLOBAL_LOG_LEVEL'] = '3'
os.environ['ASCEND_SLOG_PRINT_TO_STDOUT'] = '0'  # å‡å°‘æ—¥å¿—è¾“å‡º

def create_dataset(dataset_path, batch_size=4):
    """åˆ›å»ºæ•°æ®é›†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]
    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]
    
    transform = [
        vision.Decode(),
        vision.Resize(256),
        vision.CenterCrop(224),
        vision.Normalize(mean=mean, std=std),
        vision.HWC2CHW()
    ]

    dataset = ds.ImageFolderDataset(
        dataset_path,
        num_parallel_workers=4,
        shuffle=False
    )

    dataset = dataset.map(
        operations=transform,
        input_columns="image"
    )

    dataset = dataset.batch(batch_size, drop_remainder=True)

    return dataset

def main():
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    
    # æ„å»ºè·¯å¾„
    mindir_path = os.path.join(SCRIPT_DIR, "model", "ringmoe_model_graph.mindir")
    dataset_path = os.path.join(SCRIPT_DIR, "data", "NWPU-RESISC45", "test")
    batch_size = 4

    # 1. åˆå§‹åŒ–Liteä¸Šä¸‹æ–‡ - ä¼˜åŒ–é…ç½®
    lite_context = mslite.Context()
    lite_context.target = ["ascend"]
    lite_context.ascend.device_id = 0
    lite_context.ascend.precision_mode = "preferred_optimal"
    lite_context.ascend.auto_tune_mode = "RL,GA"
    lite_context.ascend.provider = "ge"
    
    # 2. åŠ è½½æ¨¡å‹ - æ·»åŠ é‡è¯•æœºåˆ¶
    max_retries = 3
    model = None
    
    for attempt in range(max_retries):
        try:
            model = mslite.Model()
            print(f"\nğŸ”„ æ­£åœ¨ç¼–è¯‘æ¨¡å‹(å°è¯• {attempt+1}/{max_retries})...")
            start_compile = time.time()
            model.build_from_file(mindir_path, mslite.ModelType.MINDIR, lite_context)
            compile_time = time.time() - start_compile
            print(f"âœ… æ¨¡å‹æˆåŠŸåŠ è½½! ç¼–è¯‘æ—¶é—´: {compile_time:.2f}ç§’")
            
            # æ‰“å°è¾“å…¥è¾“å‡ºä¿¡æ¯
            inputs = model.get_inputs()
            outputs = model.get_outputs()
            print(f"è¾“å…¥æ•°é‡: {len(inputs)}")
            for i, inp in enumerate(inputs):
                print(f"  è¾“å…¥[{i}]: å½¢çŠ¶={list(inp.shape)}, æ•°æ®ç±»å‹={inp.dtype}")
            print(f"è¾“å‡ºæ•°é‡: {len(outputs)}")
            for i, out in enumerate(outputs):
                print(f"  è¾“å‡º[{i}]: å½¢çŠ¶={list(out.shape)}, æ•°æ®ç±»å‹={out.dtype}")
            break
                
        except Exception as e:
            print(f"âŒ å°è¯• {attempt+1} å¤±è´¥: {str(e)}")
            if attempt == max_retries - 1:
                print("â›” æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
                print(f"1. æ¨¡å‹è·¯å¾„: {mindir_path}")
                print(f"2. Ascendç¯å¢ƒ: npu-smi æ˜¯å¦æ­£å¸¸å·¥ä½œ")
                print(f"3. æ¨¡å‹æ˜¯å¦å…¼å®¹å½“å‰Ascendç‰ˆæœ¬")
                return
            time.sleep(5)  # ç­‰å¾…åé‡è¯•

    # 3. åˆ›å»ºæ•°æ®é›†
    try:
        dataset = create_dataset(dataset_path, batch_size)
        n_batches = dataset.get_dataset_size()
        total_samples = n_batches * batch_size
        print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f" æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f" æ€»æ‰¹æ¬¡: {n_batches}")
        print(f" æ ·æœ¬æ•°é‡: {total_samples}")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}")
        return

    # 4. æ¨ç†æ‰§è¡Œ - ä¼˜åŒ–å½¢çŠ¶å¤„ç†
    correct_predictions = 0
    total_time = 0
    processed_samples = 0

    print("\nğŸš€ å¼€å§‹æ¨ç†...")
    progress_bar = tqdm(total=n_batches, desc="æ¨ç†è¿›åº¦", file=sys.stdout)

    for batch_idx, (images, labels) in enumerate(dataset.create_tuple_iterator()):
        try:
            # è½¬æ¢å¹¶ç¡®ä¿è¾“å…¥æ•°æ®æ ¼å¼æ­£ç¡®
            images_np = np.array(images.asnumpy(), dtype=np.float16)  # æ˜¾å¼è½¬æ¢ä¸ºnumpyæ•°ç»„
            
            # è·å–è¾“å…¥Tensorå¹¶å‡†å¤‡æ•°æ®
            input_tensor = model.get_inputs()[0]
            
            # æ›´å¥å£®çš„å½¢çŠ¶å¤„ç†
            if images_np.shape != tuple(input_tensor.shape):
                images_np = np.reshape(images_np, input_tensor.shape)
            
            input_tensor.set_data_from_numpy(images_np)
            
            # æ¨ç†æ‰§è¡Œ
            start_time = time.time()
            outputs = model.predict([input_tensor])
            batch_time = (time.time() - start_time) * 1000
            total_time += batch_time
            
            # å¤„ç†è¾“å‡º
            output_data = outputs[0].get_data_to_numpy()
            preds = np.argmax(output_data, axis=1)
            labels_np = labels.asnumpy()
            
            # æ›´æ–°æŒ‡æ ‡
            batch_correct = np.sum(preds == labels_np)
            correct_predictions += batch_correct
            processed_samples += len(labels_np)
            
            progress_bar.update(1)
            progress_bar.set_postfix({
                "å‡†ç¡®ç‡": f"{correct_predictions/processed_samples:.4f}",
                "é€Ÿåº¦": f"{batch_time:.2f}ms/æ‰¹æ¬¡"
            })
            
        except Exception as e:
            print(f"\nâš ï¸ æ‰¹æ¬¡ {batch_idx+1} é”™è¯¯: {str(e)}")
            continue

    progress_bar.close()

    # 5. ç»“æœç»Ÿè®¡
    if processed_samples > 0:
        final_accuracy = correct_predictions / processed_samples
        avg_time = total_time / processed_samples
        throughput = processed_samples / (total_time / 1000)
    else:
        final_accuracy = avg_time = throughput = 0

    print("\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"å¤„ç†æ ·æœ¬: {processed_samples}/{total_samples}")
    print(f"æ­£ç¡®é¢„æµ‹: {correct_predictions}")
    print(f"å‡†ç¡®ç‡: {final_accuracy:.4f}")
    print(f"å¹³å‡è€—æ—¶: {avg_time:.2f}ms/æ ·æœ¬")
    print(f"ååé‡: {throughput:.2f} æ ·æœ¬/ç§’")

if __name__ == "__main__":
    main()
