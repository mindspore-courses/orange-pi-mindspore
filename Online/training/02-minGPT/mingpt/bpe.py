"""
BPE æ˜¯â€œå­—èŠ‚å¯¹ç¼–ç å™¨â€çš„ç¼©å†™ã€‚å®ƒå°†ä»»æ„çš„ UTF-8 å­—ç¬¦ä¸²è½¬æ¢ä¸ºä¸€ç³»åˆ—æ•´æ•°ï¼Œå…¶ä¸­æ¯ä¸ªæ•´æ•°ä»£è¡¨å¸¸è§å­—ç¬¦çš„å°å—ç»„åˆã€‚
æ­¤å®ç°åŸºäº OpenAI çš„ GPT2 ç¼–ç å™¨.py æ–‡ä»¶ï¼šhttps://github.com/openai/gpt-2/blob/master/src/encoder.py
"""

import os
import json
import regex as re
import requests
import mindspore

# -----------------------------------------------------------------------------

def bytes_to_unicode():
    """
    OpenAI å°†æ¯ä¸€ä¸ªå¯èƒ½çš„å­—èŠ‚ï¼ˆå®é™…ä¸Šæ˜¯ä¸€ä¸ª 0 åˆ° 255 ä¹‹é—´çš„æ•´æ•°ï¼‰æ˜ å°„ä¸ºä¸€ä¸ªèƒ½ç›´è§‚å‘ˆç°å…¶å«ä¹‰çš„ Unicode å­—ç¬¦ã€‚
    æœ‰äº›å­—èŠ‚å› å…¶ä¸ä¼šé€ æˆä»»ä½•é—®é¢˜è€Œä¿ç•™äº†å…¶åŸæœ‰å¤–è§‚ã€‚è¿™äº›å­—èŠ‚çš„åˆ—è¡¨å®šä¹‰åœ¨ bs ä¸­ã€‚ä¾‹å¦‚ï¼š
    chr(33) è¿”å›â€œï¼â€ï¼Œå› æ­¤åœ¨è¿”å›çš„å­—å…¸ä¸­ï¼Œæˆ‘ä»¬åªéœ€æœ‰ d[33] -> â€œï¼â€ã€‚
    ç„¶è€Œï¼Œä¾‹å¦‚ chr(0) æ˜¯ "\x00"ï¼Œè¿™çœ‹èµ·æ¥å¾ˆç³Ÿç³•ã€‚
    æ‰€ä»¥ OpenAI å°†è¿™äº›å­—èŠ‚æ˜ å°„åˆ°ä¸€ä¸ªæ–°çš„å­—ç¬¦èŒƒå›´å†…ï¼Œåœ¨è¿™ä¸ªèŒƒå›´å†…ï¼Œchr() ä¼šè¿”å›ä¸€ä¸ªå•ç‹¬çš„ç¾è§‚å­—ç¬¦ã€‚
    å› æ­¤ï¼Œåœ¨æœ€ç»ˆçš„å­—å…¸ä¸­ï¼Œæˆ‘ä»¬æœ‰ d[0] -> 'Ä€'ï¼Œè€Œä¸æ˜¯ä¹‹å‰æåˆ°çš„ 'Ä€'ï¼Œè¿™æ˜¯å› ä¸º d[0] ç­‰åŒäº chr(0 + 2^8)ã€‚
    ç‰¹åˆ«åœ°ï¼Œç©ºæ ¼å­—ç¬¦æ˜¯ 32ï¼Œé€šè¿‡ ord(' ') å¯ä»¥çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚ç›¸åï¼Œæ­¤å‡½æ•°ä¼šå°†ç©ºæ ¼ï¼ˆ32ï¼‰å‘å³ç§»åŠ¨ 256 ä½åˆ° 288ï¼Œæ‰€ä»¥ d[32] -> 'Ä 'ã€‚
    æ‰€ä»¥è¿™åªæ˜¯å°† 0 åˆ° 255 è¿™äº›å­—èŠ‚ç®€å•åœ°ä¸€å¯¹ä¸€æ˜ å°„åˆ°â€œçœ‹èµ·æ¥ç¾è§‚â€çš„ Unicode å­—ç¬¦ä¸Šï¼Œ
    è¿™äº›å­—ç¬¦å¯ä»¥ä»¥å®ƒä»¬åŸå§‹å½¢å¼å‘ˆç°ï¼Œæˆ–è€…åƒ 'Ä€' æˆ– 'Ä ' è¿™æ ·çš„æœ‰è¶£ç§»ä½å­—ç¬¦å‘ˆç°ï¼Œç­‰ç­‰ã€‚
    """
    # è¿™ 188 ä¸ªæ•°å­—åŸæœ¬å°±æ˜¯æ­£å¸¸çš„ï¼Œæ— éœ€è¿›è¡Œä»»ä½•è°ƒæ•´ã€‚
    bs = list(range(ord("!"), ord("~")+1))+list(range(ord("Â¡"), ord("Â¬")+1))+list(range(ord("Â®"), ord("Ã¿")+1))
    cs = bs[:] # åœ¨è¾“å‡ºå­—å…¸ä¸­ï¼Œæ‰€æœ‰åœ¨ bs ä¸­çš„æ•´æ•° b éƒ½ä¼šç›´æ¥æ˜ å°„ä¸º chr(b) ã€‚
    # ç°åœ¨è·å–å…¶ä½™ 68 ä¸ªéœ€è¦è¿›è¡Œç§»ä½æ“ä½œçš„æ•´æ•°çš„è¡¨ç¤ºå½¢å¼
    # æ¯ä¸ªå…ƒç´ éƒ½å°†è¢«æ˜ å°„ä¸º chr(256 + n) çš„å½¢å¼ï¼Œå…¶ä¸­ n åœ¨å¾ªç¯ä¸­ä» 0 åˆ° 67 ä¾æ¬¡é€’å¢ã€‚
    n = 0
    for b in range(2**8):
        if b not in bs:
            # å¦‚æœè¿™ä¸ªå­—èŠ‚æ˜¯"ugly"çš„ï¼Œé‚£å°±å°†å…¶è½¬æ¢ä¸ºä¸‹ä¸€ä¸ªå¯ç”¨çš„"nice"çš„å­—ç¬¦ã€‚
            bs.append(b)
            cs.append(2**8+n)
            n += 1
    cs = [chr(n) for n in cs]
    d = dict(zip(bs, cs))
    return d

def get_pairs(word):
    """
    å°†æ‰€æœ‰åŒè¯ç»„åˆä»¥å…ƒç»„çš„å½¢å¼è¿”å›ï¼Œè¿™äº›å…ƒç»„åŒ…å«å¯è¿­ä»£å­—ç¬¦ä¸²ä¸­çš„è¿ç»­å…ƒç´ ã€‚
    """
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs

class Encoder:

    def __init__(self, encoder, bpe_merges):
        # å­—èŠ‚ç¼–ç å™¨/è§£ç å™¨
        self.byte_encoder = bytes_to_unicode()
        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}
        # BPE tokenç¼–ç å™¨/è§£ç å™¨
        self.encoder = encoder
        self.decoder = {v:k for k,v in self.encoder.items()}
        # BPE åˆå¹¶åˆ—è¡¨ï¼Œè¯¥åˆ—è¡¨å®šä¹‰äº† BPE çš„â€œæ ‘â€ï¼Œå…¶ä¸­åŒ…å«è‹¥å¹²å…ƒç»„ (aï¼Œ b) ï¼Œå®ƒä»¬å°†åˆå¹¶ä¸ºä¸€ä¸ªè¯é¡¹ ab ã€‚
        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))
        # ç”¨äºé¢„åˆ†è¯çš„æ‹†åˆ†æ¨¡å¼
        self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")
        self.cache = {}

    def bpe(self, token):
        """
        æ­¤å‡½æ•°åˆ©ç”¨ self.bpe_ranks å‚æ•°ï¼Œé€šè¿‡é€’å½’æ–¹å¼åœ¨æ ‘ç»“æ„ä¸­åˆå¹¶æ‰€æœ‰å¯èƒ½çš„ BPE è¯å…ƒã€‚
        è¿™é‡Œçš„â€œtokenâ€æ˜¯æŒ‡ç»è¿‡æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯å¤„ç†åçš„ä¸€ä¸ªå•ç‹¬â€œå•è¯â€ï¼ˆç»è¿‡å­—èŠ‚ç¼–ç åï¼Œä¾‹å¦‚â€œÄ thereâ€ï¼‰ã€‚
        """
        # â€œtokenâ€æ˜¯æŒ‡ä¸€ä¸ªå•ç‹¬çš„â€œå•è¯â€ç»„æˆçš„å­—ç¬¦ä¸²ï¼Œç»è¿‡å­—èŠ‚ç¼–ç åï¼Œä¾‹å¦‚â€œÄ thereâ€

        # è®°å¿†åŒ–ï¼Œä»¥æé«˜æ•ˆç‡
        if token in self.cache:
            return self.cache[token]

        word = tuple(token) # æ„æˆè¯¥æ ‡è®°çš„å„ä¸ªå­—ç¬¦ï¼Œä»¥å…ƒç»„çš„å½¢å¼å‘ˆç°
        pairs = get_pairs(word) # è·å–æ‰€æœ‰åŒè¯ç»„åˆ

        if not pairs:
            return token

        while True:

            # æ‰¾åˆ°å¯ä»¥åˆå¹¶çš„ä¸‹ä¸€ä¸ªæœ€ä½å±‚çº§çš„åŒè¯ç»„åˆ
            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))
            if bigram not in self.bpe_ranks:
                break # ä¸å†æœ‰ä¸‰å­—ç»„åˆèƒ½å¤Ÿè¢«åˆå¹¶äº†

            first, second = bigram

            # ç°åœ¨æˆ‘ä»¬å°†æŠŠåˆ—è¡¨ä¸­æ‰€æœ‰å‡ºç°çš„â€œ(ç¬¬ä¸€é¡¹ï¼Œç¬¬äºŒé¡¹)â€å†…å®¹å…¨éƒ¨æ›¿æ¢æ‰ã€‚
            # å°†å•è¯åˆå¹¶ä¸ºä¸€ä¸ªåˆå¹¶åçš„æ ‡è®° first_secondï¼Œç„¶åå°†å…¶æ·»åŠ åˆ°è¾“å‡ºåˆ—è¡¨ new_words ä¸­ã€‚
            new_word = []
            i = 0
            while i < len(word):

                # åœ¨å½“å‰å•è¯åºåˆ—ä¸­æ‰¾åˆ°â€œfirstâ€è¿™ä¸€è¯çš„ä¸‹ä¸€æ¬¡å‡ºç°ä½ç½®
                try:
                    j = word.index(first, i)
                    new_word.extend(word[i:j])
                    i = j
                except:
                    new_word.extend(word[i:])
                    break

                # å¦‚æœè¿™ç§æƒ…å†µå†æ¬¡å‘ç”Ÿï¼Œé‚£ä¹ˆå°±æŠŠè¿™ä¸¤è€…åˆå¹¶ä¸ºä¸€ä¸ªã€‚
                if word[i] == first and i < len(word)-1 and word[i+1] == second:
                    new_word.append(first+second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1

            # æ‰€æœ‰å‡ºç°çš„â€œ(ç¬¬ä¸€ï¼Œç¬¬äºŒ)â€çš„æƒ…å†µéƒ½å·²åˆå¹¶ä¸ºâ€œç¬¬ä¸€_ç¬¬äºŒâ€äº†ã€‚
            new_word = tuple(new_word)
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = get_pairs(word)

        # å°†æ‰€æœ‰å•è¯è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶ä½¿ç”¨""ä½œä¸ºåˆ†éš”ç¬¦ã€‚è¯·æ³¨æ„
        # è‡³æ­¤ï¼Œæ‰€æœ‰å­—ç¬¦å‡å·²è¿›è¡Œå­—èŠ‚ç¼–ç ï¼Œç¡®ä¿åœ¨å®é™…æ•°æ®ä¸­ä¸ä¼šä½¿ç”¨""ï¼Œè€Œå®ƒåˆ™æ˜¯ä¸€ä¸ª"special"çš„åˆ†éš”ç¬¦å­—ç¬¦ã€‚
        word = ' '.join(word)

        # cache the result and return
        self.cache[token] = word
        return word

    def encode(self, text):
        """ string goes in, list of integers comes out """
        bpe_idx = []
        # å°†è¾“å…¥æ–‡æœ¬é¢„åˆ†è¯ä¸ºå­—ç¬¦ä¸²æ ‡è®°ï¼ˆå¤§è‡´æ¥è¯´å°±æ˜¯å•è¯ï¼‰
        tokens = re.findall(self.pat, text)
        # å°†æ¯ä¸ªæ ‡è®°è½¬æ¢ä¸º BPE æ•´æ•°å½¢å¼
        for token in tokens:
            # å°†è¯¥tokenç¼–ç ä¸ºä¸€ä¸ªå­—èŠ‚ï¼ˆb' 'ï¼‰å¯¹è±¡
            token_bytes = token.encode('utf-8')
            # å°†æ‰€æœ‰å­—èŠ‚è½¬æ¢ä¸ºå…¶å¯¹åº”çš„ Unicode å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼ï¼Œå¹¶è¿›è¡Œå±•å¹³å¤„ç†ã€‚
            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)
            # æ ¹æ® self.bpe_ranks çš„å€¼æ‰§è¡Œæ‰€æœ‰é€‚ç”¨çš„ BPE åˆå¹¶æ“ä½œ
            token_merged = self.bpe(token_translated).split(' ')
            # å°†æ‰€æœ‰ BPE tokenè½¬æ¢ä¸ºæ•´æ•°
            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]
            # å°†åˆ—å‡ºçš„æ‰€æœ‰è¾“å‡ºæ•´æ•°çš„æ¸…å•è¿›ä¸€æ­¥æ‰©å……
            bpe_idx.extend(token_ix)
        return bpe_idx

    def encode_and_show_work(self, text):
        """ debugging function, same as encode but returns all intermediate work """
        bpe_idx = []
        parts = []
        tokens = re.findall(self.pat, text)
        for token in tokens:
            token_bytes = token.encode('utf-8')
            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)
            token_merged = self.bpe(token_translated).split(' ')
            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]
            bpe_idx.extend(token_ix)
            parts.append({
                'token': token,
                'token_bytes': token_bytes,
                'token_translated': token_translated,
                'token_merged': token_merged,
                'token_ix': token_ix,
            })
        out = {
            'bpe_idx': bpe_idx, # å®é™…çš„è¾“å‡ºåºåˆ—
            'tokens': tokens, # é¢„åˆ†è¯çš„ç»“æœ
            'parts': parts, # æ¯ä¸ªè¯å…ƒéƒ¨åˆ†çš„ä¸­é—´äº§ç‰©
        }
        return out

    def decode(self, bpe_idx):
        """ list of integers comes in, string comes out """
        # å°†æ•´æ•°æ˜ å°„ä¸ºç›¸åº”çš„æ ‡è®°ç¬¦
        tokens_merged = [self.decoder[token] for token in bpe_idx]
        # åè½¬å­—èŠ‚ç¼–ç å™¨ï¼Œä¾‹å¦‚å°†"Ä "æ¢å¤ä¸º" "ï¼Œä»è€Œè·å–å­—èŠ‚ã€‚
        tokens_flat = ''.join(tokens_merged)
        tokens_bytes = bytearray([self.byte_decoder[c] for c in tokens_flat])
        # æ¢å¤å®Œæ•´çš„ UTF-8 å­—ç¬¦ä¸²
        text = tokens_bytes.decode('utf-8', errors='replace')
        return text

def get_file(local_file, remote_file):
    """ downloads remote_file to local_file if necessary """
    if not os.path.isfile(local_file):
        print(f"downloading {remote_file} to {local_file}")
        response = requests.get(remote_file)
        open(local_file, "wb").write(response.content)

def get_encoder():
    """
    Returns an instance of the GPT BPE Encoder/Decoder
    and handles caching of "database" files.
    """
    home_dir = os.path.expanduser('~')
    cache_dir = os.path.join(home_dir, '.cache', 'mingpt')
    os.makedirs(cache_dir, exist_ok=True)

    # åŠ è½½åä¸ºâ€œencoder.jsonâ€çš„æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶åŒ…å«äº†ä»â€œè¯å…ƒâ€åˆ°â€œBPE ç¼–ç´¢å¼•â€çš„åŸå§‹æ˜ å°„å…³ç³»ã€‚
    encoder_local_file = os.path.join(cache_dir, 'encoder.json')
    encoder_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json'
    get_file(encoder_local_file, encoder_remote_file)
    with open(encoder_local_file, 'r') as f:
        encoder = json.load(f)
    assert len(encoder) == 50257 # 256 ä¸ªç‹¬ç«‹çš„å­—èŠ‚æ ‡è®°ï¼Œ50,000 ä¸ªåˆå¹¶æ ‡è®°ï¼Œä»¥åŠ 1 ä¸ªç‰¹æ®Šæ ‡è®°

    # åŠ è½½åä¸ºâ€œvocab.bpeâ€çš„æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶åŒ…å«äº† BPE åˆå¹¶è§„åˆ™ï¼Œå³ BPE æ ‘ç»“æ„ã€‚
    # ä»¥å…ƒç»„çš„å½¢å¼è¡¨ç¤ºï¼ˆa, bï¼‰ï¼Œè¿™è¡¨ç¤ºï¼ˆaï¼Œ bï¼‰å°†è¢«åˆå¹¶ä¸ºä¸€ä¸ªæ ‡è®°â€œabâ€
    vocab_local_file = os.path.join(cache_dir, 'vocab.bpe')
    vocab_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe'
    get_file(vocab_local_file, vocab_remote_file)
    with open(vocab_local_file, 'r', encoding="utf-8") as f:
        bpe_data = f.read()
    # light postprocessing: ç¬¬ä¸€è¡Œå’Œæœ€åä¸€è¡Œå»æ‰ç‰ˆæœ¬ä¿¡æ¯ï¼Œå…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜ã€‚
    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\n')[1:-1]]
    assert len(bpe_merges) == 50000 # 50,000 ä¸ªåˆå¹¶åçš„token

    # æ„å»ºç¼–ç å™¨å¯¹è±¡å¹¶è¿”å›
    enc = Encoder(encoder, bpe_merges)
    return enc

# -----------------------------------------------------------------------------

class BPETokenizer:
    """ ä¸€ä¸ªä¸ MindSpore ç›¸å…¼å®¹çš„ç±»ï¼Œç”¨äºå°è£…ä¸Šè¿°çš„ç¼–ç å™¨ """

    def __init__(self):
        self.encoder = get_encoder()

    def __call__(self, text, return_tensors='ms'):
        # ä»…ä½¿ç”¨ MindSporeï¼›è¿™æ˜¯å› ä¸ºå¸Œæœ›ä¸ mindnlp/mindformers çš„æ¥å£ä¿æŒä¸€è‡´ã€‚
        assert return_tensors == 'ms'
        # ç›®å‰ä»…æ”¯æŒå•ä¸ªå­—ç¬¦ä¸²è¾“å…¥ï¼Œæœªæ¥å¯èƒ½ä¼šæ”¯æŒå­—ç¬¦ä¸²åˆ—è¡¨è¾“å…¥ã€‚
        assert isinstance(text, str)
        # è¿›è¡Œç¼–ç å¹¶åˆ›å»ºä¸€ä¸ªâ€œæ‰¹æ¬¡ç»´åº¦â€å€¼ä¸º 1 çš„è®¾ç½®
        idx = [self.encoder.encode(text)]
        # è½¬æ¢ä¸º MindSpore å¼ é‡
        out = mindspore.tensor(idx, dtype=mindspore.int32)
        return out

    def decode(self, idx):
        # ç¡®ä¿ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ä¸€ç»´å¼ é‡
        assert idx.ndim == 1
        # å°†ç´¢å¼•è§£ç ä¸ºæ–‡æœ¬
        text = self.encoder.decode(idx.tolist())
        return text


if __name__ == '__main__':

    # è¿™æ˜¯ä¸ªç¼–ç ç¤ºä¾‹
    text = "Hello!! I'm Andrej Karpathy. It's 2022. w00t :D ğŸ¤—"
    e = get_encoder()
    r = e.encode_and_show_work(text)

    print("Original text is:")
    print(text)
    print("First the text gets pre-tokenized, broken up into chunks, the outcome is:")
    print(r['tokens'])
    # ['Hello', '!!', ' I', "'m", ' Andrej', ' Karpathy', '.', ' It', "'s", ' 2022', '.', ' w', '00', 't', ' :', 'D', ' ğŸ¤—']
    print("Then we iterate over each chunk and process them in turn...")
    print(r['parts'][0])
    # {'token': 'Hello', 'token_bytes': b'Hello', 'token_translated': 'Hello', 'token_merged': ['Hello'], 'token_ix': [15496]}
    print("and the final outcome is concatenating and flattening all the token_ix:")
    print(r['bpe_idx'])
    # [15496, 3228, 314, 1101, 10948, 73, 509, 5117, 10036, 13, 632, 338, 33160, 13, 266, 405, 83, 1058, 35, 12520, 97, 245]
    # è¿™æ ·ä¸€æ¥ï¼Œå®ƒå°±ä¼šæˆä¸ºè¾“å…¥åˆ°transformerä¸­çš„æ•´æ•°åºåˆ—ã€‚
    print("ready to feed into a Transformer!")
